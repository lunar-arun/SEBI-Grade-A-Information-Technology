
---

# üìò **ALGORITHMS ‚Äî Detailed Notes for SEBI Grade A IT (Phase II)**

---

# **1. INTRODUCTION**

An **algorithm** is a finite, step-by-step sequence of instructions designed to solve a problem or perform a computation.

### **Characteristics of a good algorithm**

* **Correctness:** Produces the correct output for all valid inputs.
* **Efficiency:** Uses minimal time and memory.
* **Finiteness:** Must terminate after a finite number of steps.
* **Definiteness:** Each step must be clear and unambiguous.
* **Generality:** Should work for a range of input cases.

---

# **2. SORTING ALGORITHMS**

Sorting is the process of arranging data in a particular order, typically **ascending or descending**.

Sorting helps in:

* Faster searching
* Data normalization
* Better data presentation
* Facilitating other algorithms (e.g., binary search)

---

## **2.1 CLASSIFICATION OF SORTING ALGORITHMS**

### **By comparison type**

* **Comparison-based:** Bubble sort, Quick sort, Merge sort, Heap sort, etc.
* **Non-comparison-based:** Counting sort, Radix sort, Bucket sort.

### **By memory usage**

* **In-place:** Only constant extra space (Bubble, Insertion, Selection, Quick).
* **Not in-place:** Requires extra space (Merge sort, Counting sort).

### **By stability**

* **Stable:** Maintains order of equal keys (Bubble, Insertion, Merge, Counting).
* **Unstable:** Quick sort, Heap sort, Selection sort.

---

# **COMMON SORTING ALGORITHMS**

---

## **2.2 BUBBLE SORT**

### **Idea**

Repeatedly compares adjacent elements and swaps if out of order.

### **Steps**

1. Start from first element.
2. Compare with next; swap if necessary.
3. Continue until end ‚Üí one ‚Äúpass‚Äù.
4. Repeat passes until list is sorted.

### **Time Complexity**

* Worst: **O(n¬≤)**
* Average: **O(n¬≤)**
* Best: **O(n)** (when already sorted, with optimization)

### **Space Complexity**

* **O(1)** (in-place)

### **Stability**

* **Stable**

---

## **2.3 SELECTION SORT**

### **Idea**

Select the minimum element and place it in correct position.

### **Steps**

1. Find smallest element in array.
2. Swap it with first element.
3. Repeat for remaining part of array.

### **Time Complexity**

* Worst/Average/Best: **O(n¬≤)**
  (Always scans remaining unsorted portion.)

### **Space**

* **O(1)**

### **Stable?**

* **No**

---

## **2.4 INSERTION SORT**

### **Idea**

Builds sorted portion one element at a time by inserting each element into correct position.

### **Steps**

1. Assume first element sorted.
2. Pick next element and compare backward.
3. Shift elements and insert appropriately.

### **Time Complexity**

* Worst/Average: **O(n¬≤)**
* Best: **O(n)** (already sorted)

### **Space**

* **O(1)**

### **Stable**

* **Yes**

---

## **2.5 MERGE SORT**

### **Idea**

Divide-and-conquer sorting.

### **Steps**

1. Divide array into two halves.
2. Recursively sort both halves.
3. Merge the sorted halves.

### **Time Complexity**

* Worst/Average/Best: **O(n log n)**

### **Space**

* **O(n)** (requires extra array)

### **Stable**

* **Yes**

---

## **2.6 QUICK SORT**

### **Idea**

Choose a pivot and partition array so smaller elements go to left and larger to right.

### **Steps**

1. Pick pivot (e.g., last element).
2. Partition around pivot.
3. Recursively quicksort left and right parts.

### **Time Complexity**

* Worst: **O(n¬≤)** (when array is sorted and pivot selected poorly)
* Average/Best: **O(n log n)**

### **Space**

* **O(log n)** (recursive stack)

### **Stable**

* **No**

---

## **2.7 HEAP SORT**

### **Idea**

Uses a binary heap data structure (max-heap).

### **Steps**

1. Build a max-heap.
2. Swap root with last element.
3. Reduce heap size and heapify.
4. Repeat.

### **Time Complexity**

* Worst/Average/Best: **O(n log n)**

### **Space**

* **O(1)** (in-place)

### **Stable**

* **No**

---

## **2.8 COUNTING SORT** (Non-comparison sort)

Works only for integers in a limited range.

### **Idea**

Counts occurrences of each element.

### **Time Complexity**

* **O(n + k)** (k = range of values)

### **Space**

* **O(k)**

### **Stable**

* **Yes**

---

## **Important Comparison of Sorting Algorithms**

| Sorting Algorithm | Best       | Average    | Worst      | Space    | Stable | In-place |
| ----------------- | ---------- | ---------- | ---------- | -------- | ------ | -------- |
| Bubble Sort       | O(n)       | O(n¬≤)      | O(n¬≤)      | O(1)     | Yes    | Yes      |
| Selection Sort    | O(n¬≤)      | O(n¬≤)      | O(n¬≤)      | O(1)     | No     | Yes      |
| Insertion Sort    | O(n)       | O(n¬≤)      | O(n¬≤)      | O(1)     | Yes    | Yes      |
| Merge Sort        | O(n log n) | O(n log n) | O(n log n) | O(n)     | Yes    | No       |
| Quick Sort        | O(n log n) | O(n log n) | O(n¬≤)      | O(log n) | No     | Yes      |
| Heap Sort         | O(n log n) | O(n log n) | O(n log n) | O(1)     | No     | Yes      |
| Counting Sort     | O(n+k)     | O(n+k)     | O(n+k)     | O(k)     | Yes    | No       |

---

# **3. SEARCHING ALGORITHMS**

Searching retrieves data from a data structure.

---

# **3.1 LINEAR SEARCH**

### **Idea**

Checks each element one by one.

### **Steps**

1. Start at the beginning.
2. Compare target element with each item.
3. If found, return index; else return ‚Äúnot found.‚Äù

### **Time Complexity**

* Worst/Average: **O(n)**
* Best: **O(1)** (first element)

### **Space Complexity**

* **O(1)**

### **Usage**

* Suitable for **unsorted** lists.

---

# **3.2 BINARY SEARCH**

Requires **sorted array**.

### **Idea**

Repeatedly divide search interval in half.

### **Steps**

1. Find middle element.
2. If target equals mid ‚Üí found.
3. If smaller ‚Üí search left half.
4. If larger ‚Üí search right half.

### **Time Complexity**

* Worst/Average/Best: **O(log n)**

### **Space**

* Iterative: **O(1)**
* Recursive: **O(log n)** stack space

### **Advantages**

* Much faster than linear search for large datasets.

---

# **3.3 JUMP SEARCH**

Works on sorted arrays.

### **Idea**

Jump ahead by fixed steps, then perform linear search in block.

### **Time Complexity**

* **O(‚àön)**

---

# **3.4 INTERPOLATION SEARCH**

Improved binary search when data is **uniformly distributed**.

### **Idea**

Estimates position of target based on value.

### **Time Complexity**

* Average: **O(log log n)**
* Worst: **O(n)**

---

# **3.5 HASHING (Special type of search)**

Uses hash table for near-instant search.

### **Time Complexity**

* Average: **O(1)**
* Worst: **O(n)** (due to collisions)

---

# **Comparison of Searching Algorithms**

| Searching Algorithm  | Best  | Average      | Worst    | Data Requirement              |
| -------------------- | ----- | ------------ | -------- | ----------------------------- |
| Linear Search        | O(1)  | O(n)         | O(n)     | Unsorted                      |
| Binary Search        | O(1)  | O(log n)     | O(log n) | Sorted                        |
| Jump Search          | O(‚àön) | O(‚àön)        | O(n)     | Sorted                        |
| Interpolation Search | O(1)  | O(log log n) | O(n)     | Sorted, uniformly distributed |
| Hash Search          | O(1)  | O(1)         | O(n)     | Good hash function            |

---

# 3Ô∏è‚É£ **Greedy Algorithms**

Greedy chooses the best possible choice at each step.
‚úî No backtracking
‚úî Optimal for specific problems only
‚úî Uses **Greedy Choice Property** + **Optimal Substructure**

---

## **Common Greedy Problems**

### **1. Activity Selection Problem**

* Select max number of non-overlapping activities.
* Sort by **finish time** and pick earliest.

### **2. Fractional Knapsack**

* Pick items by **value/weight ratio** until capacity is full.
* **Optimal** for fractional case.

### **3. Huffman Coding**

* Optimal prefix-free encoding based on frequency.
* Build min-heap ‚Üí combine smallest two ‚Üí repeat.

### **4. Minimum Spanning Tree**

* **Kruskal‚Äôs Algorithm:** Sort edges ‚Üí add if no cycle (Union-Find).
* **Prim‚Äôs Algorithm:** Grow tree from one node.

### **5. Dijkstra‚Äôs Algorithm**

* Shortest path from source with **non-negative edges**.
* Uses Min-Heap.

---

# 4Ô∏è‚É£ **Dynamic Programming (DP)**

DP = Recursion + Memoization / Tabulation
Uses **Optimal Substructure** and **Overlapping Subproblems**

---

## **Approaches**

### 1. **Top-Down (Memoization)**

Recursive + caching results.

### 2. **Bottom-Up (Tabulation)**

Iterative table building.

---

## **Common DP Problems**

### **1. Fibonacci / DP basics**

* Top-Down: O(n)
* Bottom-Up: O(n)

### **2. 0/1 Knapsack**

* Cannot take fractions.
* DP recurrence:
  `dp[i][w] = max(dp[i‚àí1][w], value + dp[i‚àí1][w ‚àí weight])`

### **3. Longest Common Subsequence (LCS)**

* Recurrence:
  If match: `1 + dp[i‚àí1][j‚àí1]`
  Else: `max(dp[i‚àí1][j], dp[i][j‚àí1])`

### **4. Matrix Chain Multiplication**

* Parenthesization for min cost.

### **5. Coin Change**

* Min coins OR number of ways.

### **6. Edit Distance (Levenshtein)**

* Insert, Delete, Replace.

### **7. LIS (Longest Increasing Subsequence)**

* DP: O(n¬≤)
* Binary search optimization: O(n log n)

---

# 5Ô∏è‚É£ **Backtracking**

Backtracking = DFS + pruning
‚úî Tries possibilities
‚úî Backtracks on failure
‚úî Used where greedy/DP fails

---

## **Common Backtracking Problems**

### **1. N-Queens Problem**

* Place queens so none attack.
* Use row by row placement.

### **2. Sudoku Solver**

* Fill empty cells using safe placements.

### **3. Subsets / Permutations**

* Generate power sets.

### **4. Rat in a Maze**

* Try paths recursively.

### **5. Graph Coloring**

* Assign colors without conflicts.

Backtracking Time Complexity is often **O(b^d)** (exponential).

---

# 6Ô∏è‚É£ **Divide and Conquer**

Divide ‚Üí Solve ‚Üí Combine
‚úî Efficient for recursive subproblems
‚úî Used in many core algorithms

---

## **Key Algorithms**

### **1. Merge Sort**

* Divide array ‚Üí merge sorted halves.

### **2. Quick Sort**

* Partition around pivot.

### **3. Binary Search**

* Divide search space.

### **4. Closest Pair of Points**

* Solve using geometric partitioning.

### **5. Fast Fourier Transform (FFT)**

Used for signal processing + convolution.

### **6. Karatsuba Multiplication**

* Fast integer multiplication.

---

# 7Ô∏è‚É£ **Pattern Searching Algorithms**

Used for finding substrings in a text.

---

## **1. Na√Øve Pattern Search**

* Check for match at every index.
* Time: O(n * m)

---

## **2. KMP Algorithm (Knuth-Morris-Pratt)**

* Uses **LPS (Longest Prefix Suffix)** array to skip unnecessary comparisons.
* Time: O(n + m)
* Excellent for repeated pattern searching.

---

## **3. Rabin-Karp**

* Uses hashing for pattern comparison.
* Average: O(n + m)
* Worst: O(nm) when hash collisions occur.
* Best use: multiple pattern matching (plagiarism detection).

---

## **4. Boyer-Moore Algorithm**

* Skips characters by matching from right to left.
* Uses:

  * **Bad Character Rule**
  * **Good Suffix Rule**
* Excellent for large alphabets (text search engines).

---

## **5. Z-Algorithm**

* Preprocess pattern + text into Z-array.
* Time: O(n + m)
* Efficient for substring search and pattern preprocessing.

---
