
---

# üìò **ALGORITHMS ‚Äî Detailed Notes for SEBI Grade A IT (Phase II)**

---

# **1. INTRODUCTION**

An **algorithm** is a finite, step-by-step sequence of instructions designed to solve a problem or perform a computation.

### **Characteristics of a good algorithm**

* **Correctness:** Produces the correct output for all valid inputs.
* **Efficiency:** Uses minimal time and memory.
* **Finiteness:** Must terminate after a finite number of steps.
* **Definiteness:** Each step must be clear and unambiguous.
* **Generality:** Should work for a range of input cases.

---

# **2. SORTING ALGORITHMS**

Sorting is the process of arranging data in a particular order, typically **ascending or descending**.

Sorting helps in:

* Faster searching
* Data normalization
* Better data presentation
* Facilitating other algorithms (e.g., binary search)

---

## **2.1 CLASSIFICATION OF SORTING ALGORITHMS**

### **By comparison type**

* **Comparison-based:** Bubble sort, Quick sort, Merge sort, Heap sort, etc.
* **Non-comparison-based:** Counting sort, Radix sort, Bucket sort.

### **By memory usage**

* **In-place:** Only constant extra space (Bubble, Insertion, Selection, Quick).
* **Not in-place:** Requires extra space (Merge sort, Counting sort).

### **By stability**

* **Stable:** Maintains order of equal keys (Bubble, Insertion, Merge, Counting).
* **Unstable:** Quick sort, Heap sort, Selection sort.

---

# **COMMON SORTING ALGORITHMS**

---

## **2.2 BUBBLE SORT**

### **Idea**

Repeatedly compares adjacent elements and swaps if out of order.

### **Steps**

1. Start from first element.
2. Compare with next; swap if necessary.
3. Continue until end ‚Üí one ‚Äúpass‚Äù.
4. Repeat passes until list is sorted.

### **Time Complexity**

* Worst: **O(n¬≤)**
* Average: **O(n¬≤)**
* Best: **O(n)** (when already sorted, with optimization)

### **Space Complexity**

* **O(1)** (in-place)

### **Stability**

* **Stable**

---

## **2.3 SELECTION SORT**

### **Idea**

Select the minimum element and place it in correct position.

### **Steps**

1. Find smallest element in array.
2. Swap it with first element.
3. Repeat for remaining part of array.

### **Time Complexity**

* Worst/Average/Best: **O(n¬≤)**
  (Always scans remaining unsorted portion.)

### **Space**

* **O(1)**

### **Stable?**

* **No**

---

## **2.4 INSERTION SORT**

### **Idea**

Builds sorted portion one element at a time by inserting each element into correct position.

### **Steps**

1. Assume first element sorted.
2. Pick next element and compare backward.
3. Shift elements and insert appropriately.

### **Time Complexity**

* Worst/Average: **O(n¬≤)**
* Best: **O(n)** (already sorted)

### **Space**

* **O(1)**

### **Stable**

* **Yes**

---

## **2.5 MERGE SORT**

### **Idea**

Divide-and-conquer sorting.

### **Steps**

1. Divide array into two halves.
2. Recursively sort both halves.
3. Merge the sorted halves.

### **Time Complexity**

* Worst/Average/Best: **O(n log n)**

### **Space**

* **O(n)** (requires extra array)

### **Stable**

* **Yes**

---

## **2.6 QUICK SORT**

### **Idea**

Choose a pivot and partition array so smaller elements go to left and larger to right.

### **Steps**

1. Pick pivot (e.g., last element).
2. Partition around pivot.
3. Recursively quicksort left and right parts.

### **Time Complexity**

* Worst: **O(n¬≤)** (when array is sorted and pivot selected poorly)
* Average/Best: **O(n log n)**

### **Space**

* **O(log n)** (recursive stack)

### **Stable**

* **No**

---

## **2.7 HEAP SORT**

### **Idea**

Uses a binary heap data structure (max-heap).

### **Steps**

1. Build a max-heap.
2. Swap root with last element.
3. Reduce heap size and heapify.
4. Repeat.

### **Time Complexity**

* Worst/Average/Best: **O(n log n)**

### **Space**

* **O(1)** (in-place)

### **Stable**

* **No**

---

## **2.8 COUNTING SORT** (Non-comparison sort)

Works only for integers in a limited range.

### **Idea**

Counts occurrences of each element.

### **Time Complexity**

* **O(n + k)** (k = range of values)

### **Space**

* **O(k)**

### **Stable**

* **Yes**

---

## **Important Comparison of Sorting Algorithms**

| Sorting Algorithm | Best       | Average    | Worst      | Space    | Stable | In-place |
| ----------------- | ---------- | ---------- | ---------- | -------- | ------ | -------- |
| Bubble Sort       | O(n)       | O(n¬≤)      | O(n¬≤)      | O(1)     | Yes    | Yes      |
| Selection Sort    | O(n¬≤)      | O(n¬≤)      | O(n¬≤)      | O(1)     | No     | Yes      |
| Insertion Sort    | O(n)       | O(n¬≤)      | O(n¬≤)      | O(1)     | Yes    | Yes      |
| Merge Sort        | O(n log n) | O(n log n) | O(n log n) | O(n)     | Yes    | No       |
| Quick Sort        | O(n log n) | O(n log n) | O(n¬≤)      | O(log n) | No     | Yes      |
| Heap Sort         | O(n log n) | O(n log n) | O(n log n) | O(1)     | No     | Yes      |
| Counting Sort     | O(n+k)     | O(n+k)     | O(n+k)     | O(k)     | Yes    | No       |

---
---

# **3. SEARCHING ALGORITHMS**

Searching retrieves data from a data structure.

---

# **3.1 LINEAR SEARCH**

### **Idea**

Checks each element one by one.

### **Steps**

1. Start at the beginning.
2. Compare target element with each item.
3. If found, return index; else return ‚Äúnot found.‚Äù

### **Time Complexity**

* Worst/Average: **O(n)**
* Best: **O(1)** (first element)

### **Space Complexity**

* **O(1)**

### **Usage**

* Suitable for **unsorted** lists.

---

# **3.2 BINARY SEARCH**

Requires **sorted array**.

### **Idea**

Repeatedly divide search interval in half.

### **Steps**

1. Find middle element.
2. If target equals mid ‚Üí found.
3. If smaller ‚Üí search left half.
4. If larger ‚Üí search right half.

### **Time Complexity**

* Worst/Average/Best: **O(log n)**

### **Space**

* Iterative: **O(1)**
* Recursive: **O(log n)** stack space

### **Advantages**

* Much faster than linear search for large datasets.

---

# **3.3 JUMP SEARCH**

Works on sorted arrays.

### **Idea**

Jump ahead by fixed steps, then perform linear search in block.

### **Time Complexity**

* **O(‚àön)**

---

# **3.4 INTERPOLATION SEARCH**

Improved binary search when data is **uniformly distributed**.

### **Idea**

Estimates position of target based on value.

### **Time Complexity**

* Average: **O(log log n)**
* Worst: **O(n)**

---

# **3.5 HASHING (Special type of search)**

Uses hash table for near-instant search.

### **Time Complexity**

* Average: **O(1)**
* Worst: **O(n)** (due to collisions)

---

# **Comparison of Searching Algorithms**

| Searching Algorithm  | Best  | Average      | Worst    | Data Requirement              |
| -------------------- | ----- | ------------ | -------- | ----------------------------- |
| Linear Search        | O(1)  | O(n)         | O(n)     | Unsorted                      |
| Binary Search        | O(1)  | O(log n)     | O(log n) | Sorted                        |
| Jump Search          | O(‚àön) | O(‚àön)        | O(n)     | Sorted                        |
| Interpolation Search | O(1)  | O(log log n) | O(n)     | Sorted, uniformly distributed |
| Hash Search          | O(1)  | O(1)         | O(n)     | Good hash function            |

---
---

# 3Ô∏è‚É£ **Greedy Algorithms**

Greedy chooses the best possible choice at each step.
‚úî No backtracking
‚úî Optimal for specific problems only
‚úî Uses **Greedy Choice Property** + **Optimal Substructure**

### **Key Idea:**

> At each step, pick the best possible option without worrying about future consequences.

---

# **3. Characteristics of Greedy Algorithms**

### **3.1 Greedy Choice Property**

A problem possesses the greedy choice property if:

* A global optimum can be achieved by choosing a local optimum at every step.

### **3.2 Optimal Substructure**

A problem has optimal substructure if:

* The optimal solution contains optimal solutions to its subproblems.

If a problem has both **greedy-choice property** AND **optimal substructure**, then a greedy algorithm may produce an optimal solution.

---

# **4. Steps in Designing a Greedy Algorithm**

1. **Understand the problem** (objective function)
2. **Identify possible choices** at each step
3. **Define a greedy strategy**:

   * What is the best possible choice at each step?
4. **Check Greedy Choice Property**
5. **Check Optimal Substructure**
6. **Prove correctness** (optional but important for theoretical work)
7. **Implement the Algorithm**

---

# **5. Advantages and Disadvantages**

### **Advantages**

* Very simple to understand and implement
* Provides fast solutions (usually O(n log n) or O(n))
* Works well for many optimization problems

### **Disadvantages**

* Doesn‚Äôt always provide the optimal solution
* Requires careful proof that greedy choice works
* Dynamic Programming may be needed when Greedy fails

---

# **6. Common Problems Solved Using Greedy Algorithms**

### **6.1 Activity Selection Problem**

Goal: Select maximum number of non-overlapping activities based on start & finish times.

**Greedy Choice:**
Select the activity that ends earliest.

**Reason:**
This frees up time for more activities.

---

### **6.2 Fractional Knapsack Problem**

Goal: Maximize profit with limited weight capacity, and you can take fractional items.

**Greedy Choice:**
Choose items based on **maximum value/weight ratio**.

**Note:**
Greedy works for *fractional knapsack*, but fails for *0/1 knapsack* (DP needed).

---

### **6.3 Huffman Coding**

Used for data compression.

**Greedy Choice:**
Build a tree by repeatedly combining two smallest frequency nodes.

**Result:**
Minimum prefix code ‚Üí optimal compression.

---

### **6.4 Minimum Spanning Tree (MST)**

Two greedy algorithms are used:

#### **(a) Kruskal‚Äôs Algorithm**

* Sort edges by increasing weight
* Add smallest edges without creating cycles

#### **(b) Prim‚Äôs Algorithm**

* Start from any node
* Add smallest edge connecting tree to new vertex

Both are optimal and greedy.

---

### **6.5 Dijkstra‚Äôs Shortest Path Algorithm**

Goal: Find shortest path from source to other nodes in a graph with non-negative weights.

**Greedy Choice:**
Pick the vertex with the smallest tentative distance.

---

### **6.6 Job Sequencing with Deadlines**

Goal: Maximize profit by scheduling jobs before deadlines.

**Greedy Choice:**
Sort jobs by decreasing profit and schedule the most profitable jobs first.

---

### **6.7 Coin Change Problem (Greedy Version)**

Goal: Find minimum number of coins for a given amount.

**Greedy Choice:**
Select the largest denomination coin each time.

**Note:**
Works only for canonical coin systems (like Indian or US currency).
Fails for arbitrary coin values ‚Üí dynamic programming needed.

---
---

# 4Ô∏è‚É£ **Dynamic Programming (DP)**

**Dynamic Programming (DP)** is an algorithmic technique used to solve **complex problems by breaking them into simpler overlapping subproblems**, solving each subproblem once, and **storing the result for future use**.


It is especially useful when:

* Problems have **optimal substructure**: The optimal solution can be constructed from optimal solutions of subproblems.
* Problems have **overlapping subproblems**: The same subproblems appear multiple times.

DP is essentially **a trade-off of time for space**: By storing results, we avoid unnecessary recomputation, achieving significant speed-ups.

DP = Recursion + Memoization / Tabulation
Uses **Optimal Substructure** and **Overlapping Subproblems**

---


# **2. Why Use Dynamic Programming?**

Many brute-force or naive recursive solutions repeatedly compute the same results, causing exponential time complexity.

Dynamic programming reduces this by:

* Avoiding repetition
* Reusing previously-computed results

This often reduces time complexity from **exponential O(2‚Åø)** to **polynomial or linear O(n)**.

---

# **3. Key Concepts in Dynamic Programming**

### **(a) Optimal Substructure**

A problem has optimal substructure if:

* The optimal solution can be built from the **optimal solutions of its subproblems**.

**Example:**
Shortest path in a graph ‚Äî shortest path from A to C via B = shortest(A‚ÜíB) + shortest(B‚ÜíC).

### **(b) Overlapping Subproblems**

A problem has overlapping subproblems if:

* It repeatedly solves the same subproblem.

**Example:**
Fibonacci recursion recomputes the same values many times.

### **(c) Memoization vs Tabulation**

Dynamic programming has two main approaches:

---

# **4. Types of Dynamic Programming**

## **I. Top-Down Approach (Memoization)**

* Uses **recursion**
* Stores computed results in a **hash map or array**
* Solves from **larger problem ‚Üí smaller subproblems**
* Only computes necessary subproblems

**Example (Fibonacci using memoization):**

```python
def fib(n, memo={}):
    if n <= 1:
        return n
    if n in memo:
        return memo[n]
    memo[n] = fib(n-1, memo) + fib(n-2, memo)
    return memo[n]
```

---

## **II. Bottom-Up Approach (Tabulation)**

* Uses **iteration**
* Builds a **table (array)** from smallest subproblems ‚Üí large
* No recursion needed

**Example (Fibonacci using tabulation):**

```python
def fib(n):
    dp = [0]*(n+1)
    dp[1] = 1
    for i in range(2, n+1):
        dp[i] = dp[i-1] + dp[i-2]
    return dp[n]
```

---

# **5. When Do We Apply DP?**

Use DP when:

1. Problem involves **choices** (e.g., take or skip)
2. Subproblems repeat
3. You can define a state and recurrence relation

Example patterns:

* **Knapsack problems**
* **Shortest paths** (Bellman-Ford, Floyd‚ÄìWarshall)
* **Counting paths** (grid walking)
* **String problems** (LCS, Edit Distance)
* **Partition problems**
* **Scheduling problems**

---

# **6. How to Design a DP Algorithm (Step-by-Step)**

### **Step 1: Identify the states**

Define what variables describe a subproblem.

Example:
For knapsack: `dp[i][w]` = best value using first *i* items with weight *w*.

### **Step 2: Write the recurrence relation**

Describe how the answer depends on smaller subproblems.

Example:

```
dp[i][w] = max( dp[i-1][w], value[i] + dp[i-1][w-weight[i]] )
```

### **Step 3: Identify base cases**

Example:

```
dp[0][w] = 0
dp[i][0] = 0
```

### **Step 4: Choose memoization or tabulation**

* Small recursion depth ‚Üí memoization
* Large iterative fill ‚Üí tabulation

### **Step 5: Compute the answer**

The final cell gives the answer.

---

# **7. Popular Dynamic Programming Examples**

### **(1) Fibonacci Numbers**

* Simplest example of overlapping subproblems.

### **(2) Knapsack Problem (0/1 and Fractional)**

Determine maximum profit with weight constraints.

### **(3) Longest Common Subsequence (LCS)**

Used in DNA sequence alignment or diff tools.

### **(4) Edit Distance (Levenshtein Distance)**

Measures similarity between strings.

### **(5) Matrix Chain Multiplication**

Optimal parenthesization of matrix multiplication.

### **(6) Coin Change Problem**

Find minimum coins needed to make a sum.

### **(7) Shortest Path ‚Äì Floyd‚ÄìWarshall**

All-pairs shortest path using DP.

### **(8) Catalan Numbers**

Counts valid parenthesizations, BSTs, etc.

---
---

# 5Ô∏è‚É£ **Backtracking**

Backtracking = DFS + pruning
‚úî Tries possibilities
‚úî Backtracks on failure
‚úî Used where greedy/DP fails

---

## **Common Backtracking Problems**

### **1. N-Queens Problem**

* Place queens so none attack.
* Use row by row placement.

### **2. Sudoku Solver**

* Fill empty cells using safe placements.

### **3. Subsets / Permutations**

* Generate power sets.

### **4. Rat in a Maze**

* Try paths recursively.

### **5. Graph Coloring**

* Assign colors without conflicts.

Backtracking Time Complexity is often **O(b^d)** (exponential).

---
---


---

# **Divide and Conquer ‚Äì Full Detailed Explanation**

## **1. What Is Divide and Conquer?**

**Divide and Conquer** is a fundamental algorithmic technique that solves a large problem by:

1. **Dividing** the problem into smaller subproblems
2. **Conquering** each subproblem recursively
3. **Combining** the solutions of subproblems to get the final result

It is extremely powerful for problems that can be broken down into independent sub-tasks.

---

# **2. Why Use Divide and Conquer?**

Divide and Conquer:

* Reduces problem size at each step
* Often results in logarithmic or near-linear time complexities
* Works well for large inputs
* Many efficient algorithms (like sorting) use this technique

---

# **3. Steps in Divide and Conquer**

### ‚úî **Step 1: Divide**

Break the main problem into smaller subproblems of the same type.
Example: Split an array into two halves.

### ‚úî **Step 2: Conquer**

Solve each subproblem recursively.
If the subproblem size becomes small enough ‚Üí solve directly (base case).

### ‚úî **Step 3: Combine**

Merge the subproblem results to form the final solution.

This pattern gives the classic recurrence:

```
T(n) = aT(n/b) + f(n)
```

Where:

* `a` = number of subproblems
* `n/b` = size of each subproblem
* `f(n)` = combining time

---

# **4. Examples of Divide and Conquer Algorithms**

## **1. Merge Sort**

* **Divide**: Split array into two halves
* **Conquer**: Recursively sort both halves
* **Combine**: Merge two sorted halves
* **Time Complexity**: O(n log n)

## **2. Quick Sort**

* **Divide**: Partition array using a pivot
* **Conquer**: Recursively apply quicksort on left and right partitions
* **Combine**: No major combine step (in-place)
* **Average Time**: O(n log n)
* **Worst Time**: O(n¬≤)

## **3. Binary Search**

* **Divide**: Choose mid element, decide whether to search left or right
* **Conquer**: Search in one half
* **Combine**: No combination needed
* **Time**: O(log n)

## **4. Strassen‚Äôs Matrix Multiplication**

* Break matrices into smaller sub-matrices
* Recursively multiply using 7 multiplications instead of 8
* Improves over naive O(n¬≥)

## **5. Closest Pair of Points Problem**

* Sort points and divide into halves
* Recursively solve
* Combine by checking middle strip
* **Time**: O(n log n)

## **6. Karatsuba Multiplication**

* Divide numbers into halves
* Uses 3 multiplications instead of 4
* Faster large integer multiplication

---

# **5. Real-World Applications**

### **Data Processing**

* Efficient sorting algorithms (used in libraries)
* Searching large databases

### **Computer Graphics**

* Ray tracing
* Quadtree and Octree decomposition

### **Machine Learning**

* Decision trees
* K-d trees for nearest neighbor search

### **Scientific Computing**

* Fast Fourier Transform (FFT)
* Solving differential equations

### **Parallel Computing**

* Many divide-and-conquer algorithms can run in parallel
  (since subproblems are independent)

---

# **6. Complexity Analysis Using the Master Theorem**

For recurrence:

```
T(n) = aT(n/b) + f(n)
```

The **Master Theorem** provides solutions:

### **Case 1: f(n) = O(n^c) where c < log_b(a)**

‚Üí Subproblem dominates
‚Üí **T(n) = Œò(n^(log_b(a)))**

### **Case 2: f(n) = Œò(n^(log_b(a)) * log^k(n))**

‚Üí Both are balanced
‚Üí **T(n) = Œò(n^(log_b(a)) * log^(k+1)(n))**

### **Case 3: f(n) = Œ©(n^c) where c > log_b(a)**

‚Üí Combine work dominates
‚Üí **T(n) = Œò(f(n))**

This theorem helps quickly analyze divide-and-conquer algorithms.

---

# **7. Advantages of Divide and Conquer**

‚úî Reduces complexity significantly
‚úî Problems get easier as they are divided
‚úî Naturally suited for recursion
‚úî Many algorithms achieve optimal performance
‚úî Subproblems can run **in parallel**

---

# **8. Disadvantages of Divide and Conquer**

‚úñ Overhead of recursion
‚úñ Not efficient if combining step is expensive
‚úñ Extra memory usage (like merge sort)
‚úñ Some problems do not split evenly ‚Üí performance drops (e.g., poor pivot in quicksort)

---

# **9. Typical Divide and Conquer Pseudocode**

```pseudo
function divideAndConquer(problem):
    if problem is small:
        return base case solution
    
    subProblem1, subProblem2 = divide(problem)
    
    solution1 = divideAndConquer(subProblem1)
    solution2 = divideAndConquer(subProblem2)
    
    return combine(solution1, solution2)
```

---

# **10. Divide and Conquer vs Dynamic Programming vs Backtracking**

| Feature      | Divide & Conquer           | Dynamic Programming    | Backtracking        |
| ------------ | -------------------------- | ---------------------- | ------------------- |
| Subproblems  | Independent                | Overlapping            | Constraint-based    |
| Combine step | Yes                        | Sometimes              | No (just exploring) |
| Recursion    | Always                     | Often                  | Always              |
| Output       | One optimal result         | Optimal result         | All valid results   |
| Efficiency   | Usually fast (logarithmic) | Very fast (polynomial) | Slow (exponential)  |


---
---

# 7Ô∏è‚É£ **Pattern Searching Algorithms**

Used for finding substrings in a text.

---

## **1. Na√Øve Pattern Search**

* Check for match at every index.
* Time: O(n * m)

---

## **2. KMP Algorithm (Knuth-Morris-Pratt)**

* Uses **LPS (Longest Prefix Suffix)** array to skip unnecessary comparisons.
* Time: O(n + m)
* Excellent for repeated pattern searching.

---

## **3. Rabin-Karp**

* Uses hashing for pattern comparison.
* Average: O(n + m)
* Worst: O(nm) when hash collisions occur.
* Best use: multiple pattern matching (plagiarism detection).

---

## **4. Boyer-Moore Algorithm**

* Skips characters by matching from right to left.
* Uses:

  * **Bad Character Rule**
  * **Good Suffix Rule**
* Excellent for large alphabets (text search engines).

---

## **5. Z-Algorithm**

* Preprocess pattern + text into Z-array.
* Time: O(n + m)
* Efficient for substring search and pattern preprocessing.

---
